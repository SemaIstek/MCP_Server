{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dab39c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading CSV file: ../assets/filtered_data.csv\n",
      "üìä Rows: 2302, Columns: 9\n",
      "üìö Created 2302 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_191071/4202379530.py:41: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_191071/4202379530.py:61: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2:1b\")\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1Ô∏è‚É£ Import libraries\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è‚É£ Load CSV\n",
    "# -----------------------------\n",
    "csv_path = \"../assets/filtered_data.csv\"\n",
    "\n",
    "print(f\"üìÅ Loading CSV file: {csv_path}\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"üìä Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è‚É£ Create Documents (handle NaN)\n",
    "# -----------------------------\n",
    "documents = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Replace NaN with \"N/A\"\n",
    "    values = [str(v) if pd.notna(v) else \"N/A\" for v in row.values]\n",
    "    content = \" | \".join(values)\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content=content,\n",
    "            metadata={\"row_id\": idx}\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"üìö Created {len(documents)} documents\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4Ô∏è‚É£ Embedding model\n",
    "# -----------------------------\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5Ô∏è‚É£ Create FAISS vector store\n",
    "# -----------------------------\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=[doc.page_content for doc in documents],\n",
    "    embedding=embeddings,\n",
    "    metadatas=[doc.metadata for doc in documents]\n",
    ")\n",
    "print(\"‚úÖ Vector store created successfully\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6Ô∏è‚É£ Create Retriever\n",
    "# -----------------------------\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# -----------------------------\n",
    "# 7Ô∏è‚É£ Setup LLM\n",
    "# -----------------------------\n",
    "llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8Ô∏è‚É£ Define Prompt Template\n",
    "# -----------------------------\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an assistant that answers questions using ONLY the CSV data.\n",
    "\n",
    "CSV DATA:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "If the answer is not in the CSV, say:\n",
    "\"I cannot find this information in the CSV.\"\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 9Ô∏è‚É£ Define RAG function\n",
    "# -----------------------------\n",
    "def ask(question: str):\n",
    "    # Use get_relevant_texts to avoid AttributeError\n",
    "    docs = retriever(question)  # bu bir liste d√∂nd√ºr√ºyor\n",
    "    context = \"\\n\".join(d.page_content for d in docs)\n",
    "    return llm.invoke(prompt.format(context=context, question=question))\n",
    "\n",
    "# ------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8aa7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the vectorstore directly\n",
    "def ask(question: str):\n",
    "    # FAISS object has a method: similarity_search\n",
    "    docs = vectorstore.similarity_search(question, k=5)  # top 5 most similar docs\n",
    "    context = \"\\n\".join(d.page_content for d in docs)\n",
    "    return llm.invoke(prompt.format(context=context, question=question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5338a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Based on the provided CSV data, it appears that \"information\" can be broken down into several categories:\n",
      "\n",
      "1. **Location**: The columns \"Natural\", \"Sint Maarten (Dutch part)\", \"SXM\", and \"IDN\" contain location names or codes for various geographic regions.\n",
      "2. **Event/Catagory**: Columns like \"Storm\", \"Volcanic activity\", and \"Flood\" store information about different types of events or phenomena.\n",
      "3. **Year/Date**: The columns \"2017\", \"2018\", \"2022\", and \"2014\" indicate the year or date for each occurrence.\n",
      "\n",
      "These categories can be grouped into three main areas:\n",
      "\n",
      "1. Geographic Information (Location)\n",
      "2. Event/Catagory\n",
      "3. Date/Time (Year or Date)\n"
     ]
    }
   ],
   "source": [
    "# Test RAG system\n",
    "answer = ask(\"What information is stored in this dataset?\")\n",
    "print(f\"Answer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74164f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "To determine which country suffered the most damage, I will calculate the total \"Damage\" for each country based on the given data.\n",
      "\n",
      "First, I'll sum up all the values for \"Natural\" in the columns with unique countries:\n",
      "\n",
      "- India: 226806 + 197798 = 424604\n",
      "- Brazil: 197798 + 17897 = 216985\n",
      "- China: 99168 + 1018663 = 1029921\n",
      " \n",
      "\n",
      "Now, I'll sum up all the values for \"Mass movement (wet)\" in the columns with unique countries:\n",
      "\n",
      "- India: 8569.902547 + 3529081.3 = 3604170.802847\n",
      "- Brazil: 2824.715413 + 317756.5 = 306080.215913\n",
      "- China: 1053.112314 + 8474922.7 = 8485766.923\n",
      "\n",
      "Next, I'll compare the total \"Damage\" for each country to find out which one suffered the most damage:\n",
      "\n",
      "India had the highest total damage at 424604 units.\n",
      "Brazil and China also had a significant amount of damage but are tied as they have higher totals than India, with Brazil having 3604170.80 more units in damages than India.\n",
      "\n",
      "Therefore, I cannot find this information in the CSV.\n"
     ]
    }
   ],
   "source": [
    "answer = ask(\"Which country suffered the most damage?\")\n",
    "print(f\"Answer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7a432e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "To answer this question, we need to look for a column with 'Switzerland' or any reference to Switzerland.\n",
      "\n",
      "Looking at the data:\n",
      "\n",
      "- Natural | Glacial lake outburst flood | India | IND | 234 | 226806 | 2021 | 2238.127142 | N/A\n",
      "- Mass movement (wet) | Indonesia | Sierra Leone | SLE | 1102 | 35818 | 2017 | 484.4561288 | 1120.0\n",
      "\n",
      "There is no 'Switzerland' or any direct reference to Switzerland in the provided CSV data.\n",
      "\n",
      "Therefore, I cannot find this information in the CSV.\n"
     ]
    }
   ],
   "source": [
    "answer = ask(\"How much damage did Switzerland suffer?\")\n",
    "print(f\"Answer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd1b3a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "SELECT SUM(Total) FROM Natural\n"
     ]
    }
   ],
   "source": [
    "answer = ask(\"Summarize the total damage for all countries.\")\n",
    "print(f\"Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4b8956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Vector database folder: ./vector_db\n",
      "‚úÖ Chroma vector store created at: ./vector_db/chroma_db\n",
      "Answer (Chroma):\n",
      "From the provided CSV data:\n",
      "\n",
      "- The first column 'Natural' likely represents a natural disaster type (e.g., Flood, Earthquake, Storm).\n",
      "- The second column 'Flood' contains flood-related data.\n",
      "- The third column 'Viet Nam' appears to be related to Vietnam or Southeast Asia.\n",
      "- The fourth column 'VNM' is the Vietnamese acronym for Vietnam.\n",
      "- The fifth column '1' indicates a single entry in each row.\n",
      "- The sixth column '424' seems to hold some value, but its meaning isn't clear without additional context.\n",
      "- The seventh column '2017' contains information about the year of data collection or the event's occurrence in 2017.\n",
      "- The eighth column '2992.071532' likely holds a specific measurement related to events in Vietnam.\n",
      "- The ninth column '229877.4' is also related to Vietnam and seems to hold some numerical value, possibly indicating area measurements.\n",
      "- The tenth column '1' again indicates a single entry per row.\n",
      "- The eleventh column '424' appears to be an identifier or code for each data point in the dataset.\n",
      "- The twelfth column '2018' contains information about another year (or event) with a similar numerical value as before, possibly referring to Vietnam.\n",
      "- The thirteenth column '3902.661676' holds a measurement related to events in Indonesia.\n",
      "- The fourteenth column '38834.05293' is also related to specific measurements for Indonesia.\n",
      "- The fifteenth column 'Germany' represents the country of origin (or Germany) for some data points, including the storms mentioned.\n",
      "- The sixteenth column '2' indicates a single entry per row.\n",
      "- The seventeenth column '596963' appears to contain measurement values, likely related to Japan's natural disasters or environmental changes.\n",
      "- The eighteenth column 'Strom' seems like an abbreviation or shortened form of 'Storm', possibly for storms in other countries (or the same country but not Germany).\n",
      "- The nineteenth column 'Japan' represents another country with its respective data points or measurements.\n",
      "- The twentieth column '2' indicates a single entry per row, likely referring to additional information about Japan's natural disasters or environmental changes.\n",
      "\n",
      "Therefore, the dataset appears to store various pieces of geographical and meteorological information related to different countries, including Vietnam, Indonesia, Japan, and Germany.\n"
     ]
    }
   ],
   "source": [
    "# Create vector_db folder to store databases\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "vector_db_dir = \"./vector_db\"\n",
    "Path(vector_db_dir).mkdir(exist_ok=True)\n",
    "print(f\"üìÅ Vector database folder: {vector_db_dir}\")\n",
    "\n",
    "# Alternative: Using Chroma instead of FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Create Chroma vector store with persistent storage\n",
    "chroma_path = os.path.join(vector_db_dir, \"chroma_db\")\n",
    "chroma_vectorstore = Chroma.from_texts(\n",
    "    texts=[doc.page_content for doc in documents],\n",
    "    embedding=embeddings,\n",
    "    metadatas=[doc.metadata for doc in documents],\n",
    "    persist_directory=chroma_path\n",
    ")\n",
    "print(f\"‚úÖ Chroma vector store created at: {chroma_path}\")\n",
    "\n",
    "# Define RAG function using Chroma\n",
    "def ask_with_chroma(question: str):\n",
    "    docs = chroma_vectorstore.similarity_search(question, k=5)\n",
    "    context = \"\\n\".join(d.page_content for d in docs)\n",
    "    return llm.invoke(prompt.format(context=context, question=question))\n",
    "\n",
    "# Test with Chroma\n",
    "answer_chroma = ask_with_chroma(\"What information is stored in this dataset?\")\n",
    "print(f\"Answer (Chroma):\\n{answer_chroma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca30d19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Let's analyze the data.\n",
      "\n",
      "According to the CSV data, China suffered the most damage with a total mass movement of 1018663. This corresponds to the entry in Brazil, where the mass movement was 17897 (as given by the question).\n",
      "\n",
      "Therefore, I can confidently answer that:\n",
      "\"I cannot find this information in the CSV.\"\n"
     ]
    }
   ],
   "source": [
    "answer = ask_with_chroma(\"Which country suffered the most damage?\")\n",
    "print(f\"Answer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e046a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "To find the top 3 countries with the lowest total damage, we need to calculate the total damage for each country and then rank them. \n",
      "\n",
      "Here are the calculations:\n",
      "\n",
      "1. Natural | Earthquake | Japan | JPN | \n",
      "2. Natural | Earthquake | China | CHN |\n",
      "3. Natural | Flood | Viet Nam | VNM |\n",
      "\n",
      "Total Damage:\n",
      "1. 752000 + 12720.21632 = 862220.21632\n",
      "2. 42000 + 10359.14986 = 52459.14986\n",
      "3. 0 + 3703.649059 = 3703.649059\n",
      "\n",
      "Ranking by Total Damage:\n",
      "1. Japan - 862220.21632\n",
      "2. China - 52459.14986\n",
      "3. Viet Nam - 3703.649059\n",
      "\n",
      "Top 3 countries with the lowest total damage are: Japan, China, and Viet Nam.\n",
      "\n",
      "However, I couldn't find any information about a drought in Marshall Islands. The CSV data doesn't include that event, so it's not included in our calculation.\n",
      "\n",
      "Therefore, the answer is:\n",
      "I cannot find this information in the CSV.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_with_chroma(\"What are the top 3 countries with the lowest total damage?\")\n",
    "print(f\"Answer:\\n{answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
